import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, stddev, lit, expr
from pyspark.sql.window import Window
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegressionModel
import pandas as pd
import datetime
import os

# ğŸ“Œ Initialisation de la session Spark
spark = SparkSession.builder.appName("StockAnalysis").getOrCreate()

# ğŸ“Œ Chargement des donnÃ©es historiques
print("ğŸ“Œ Chargement des donnÃ©es historiques...")
df = spark.read.option("header", "true").option("inferSchema", "true").csv("../data/financial_data_cleaned.csv")

# ğŸ“Œ SÃ©lectionner uniquement les colonnes utiles et partitionner les donnÃ©es par Date pour amÃ©liorer les performances
df_selected = df.select("Date", "Close_AAPL", "Close_TSLA", "Close_GOOGL", "Close_MSFT")
df_selected = df_selected.withColumn("Date", col("Date").cast("date"))
df_selected = df_selected.repartition(10, "Date")  # Partitionner par Date

# ğŸ“Œ Mettre en cache les donnÃ©es pour Ã©viter des recalculs
df_selected.cache()

# ğŸ“Œ DÃ©finition des fenÃªtres Spark
windowRolling = Window.partitionBy("Date").orderBy("Date").rowsBetween(-5, 0)

# ğŸ“Œ Calcul des indicateurs SMA et VolatilitÃ©
print("ğŸ“Œ Calcul des indicateurs SMA et VolatilitÃ©...")
df_features = df_selected.withColumn("SMA_AAPL", avg("Close_AAPL").over(windowRolling)) \
                         .withColumn("SMA_TSLA", avg("Close_TSLA").over(windowRolling)) \
                         .withColumn("SMA_GOOGL", avg("Close_GOOGL").over(windowRolling)) \
                         .withColumn("SMA_MSFT", avg("Close_MSFT").over(windowRolling)) \
                         .withColumn("Volatility_AAPL", stddev("Close_AAPL").over(windowRolling)) \
                         .withColumn("Volatility_TSLA", stddev("Close_TSLA").over(windowRolling)) \
                         .withColumn("Volatility_GOOGL", stddev("Close_GOOGL").over(windowRolling)) \
                         .withColumn("Volatility_MSFT", stddev("Close_MSFT").over(windowRolling))

df_clean = df_features.fillna(0)

# ğŸ“Œ Charger les modÃ¨les
models = {}
stocks = ["AAPL", "TSLA", "GOOGL", "MSFT"]

for stock in stocks:
    model_path = f"../models/stock_price_model_{stock}"
    if os.path.exists(model_path):
        print(f"ğŸ“Œ Chargement du modÃ¨le pour {stock}...")
        models[stock] = LinearRegressionModel.load(model_path)
    else:
        print(f"âŒ ERREUR : Le modÃ¨le {model_path} est introuvable.")

# ğŸ“Œ GÃ©nÃ©rer les futures dates (Mars 2025 - Mars 2026)
print("ğŸ“Œ GÃ©nÃ©ration des dates futures...")
last_date = df_selected.agg({"Date": "max"}).collect()[0][0]
future_dates = [last_date + datetime.timedelta(days=i) for i in range(1, 366)]  # 1 an de prÃ©visions
future_df = spark.createDataFrame([(d,) for d in future_dates], ["Date"])

# ğŸ“Œ VÃ©rifier si les donnÃ©es historiques ne sont pas vides
if not df_clean.rdd.isEmpty():
    last_known_values = df_clean.orderBy(col("Date").desc()).limit(1).collect()[0]

    for stock in stocks:
        future_df = future_df.withColumn(f"SMA_{stock}", lit(last_known_values[f"SMA_{stock}"])) \
                             .withColumn(f"Volatility_{stock}", lit(last_known_values[f"Volatility_{stock}"]))

    # ğŸ“Œ GÃ©nÃ©rer SMA et VolatilitÃ© en fonction des valeurs prÃ©cÃ©dentes pour Ã©viter les prÃ©dictions constantes
    future_df = future_df.withColumn("SMA_AAPL", expr("SMA_AAPL * (1 + rand() * 0.02 - 0.01)")) \
                         .withColumn("Volatility_AAPL", expr("Volatility_AAPL * (1 + rand() * 0.02 - 0.01)")) \
                         .withColumn("SMA_TSLA", expr("SMA_TSLA * (1 + rand() * 0.02 - 0.01)")) \
                         .withColumn("Volatility_TSLA", expr("Volatility_TSLA * (1 + rand() * 0.02 - 0.01)")) \
                         .withColumn("SMA_GOOGL", expr("SMA_GOOGL * (1 + rand() * 0.02 - 0.01)")) \
                         .withColumn("Volatility_GOOGL", expr("Volatility_GOOGL * (1 + rand() * 0.02 - 0.01)")) \
                         .withColumn("SMA_MSFT", expr("SMA_MSFT * (1 + rand() * 0.02 - 0.01)")) \
                         .withColumn("Volatility_MSFT", expr("Volatility_MSFT * (1 + rand() * 0.02 - 0.01)"))
else:
    print("âŒ ERREUR : Les donnÃ©es historiques sont vides. Impossible d'extrapoler les SMA et VolatilitÃ©.")

# ğŸ“Œ Appliquer la prÃ©diction pour chaque action
for stock in stocks:
    if stock in models:
        print(f"ğŸ“Œ PrÃ©diction pour {stock}...")

        assembler = VectorAssembler(inputCols=[f"SMA_{stock}", f"Volatility_{stock}"], outputCol="features", handleInvalid="keep")
        df_spark_pred = assembler.transform(future_df).select("Date", "features")

        predictions = models[stock].transform(df_spark_pred).select("Date", "prediction")
        predictions = predictions.withColumnRenamed("prediction", f"Prediction_{stock}")

        # ğŸ“Œ Fusionner les prÃ©dictions avec `future_df`
        future_df = future_df.join(predictions, on="Date", how="left")

# ğŸ“Œ Sauvegarder les prÃ©dictions
df_predictions = future_df.toPandas()
df_predictions.to_parquet("../results/predictions.parquet", index=False)

print("âœ… PrÃ©dictions terminÃ©es et enregistrÃ©es dans `../results/predictions.parquet`")
